> The following are the commands and scripts that can be  used for data
> querying and download of both Landsat and Sentinel imagery. Before
> using this, you should first install the necessary software from the
> "Software Installation" component of this guide.

<br>

- [1. Download metadata of imagery](#1-download-metadata-of-imagery)
  * [1.1. Download metadata of Landsat imagery (no longer functional)](#11-download-metadata-of-landsat-imagery-no-longer-functional)
  * [1.2. Download metadata of Sentinel imagery](#12-download-metadata-of-sentinel-imagery)
- [2. R script for viewing and understanding Landsat and Sentinel metadata](#2-r-script-for-viewing-and-understanding-landsat-and-sentinel-metadata)
- [3. Display information about satellite imagery in terminal and download download links](#3-display-information-about-satellite-imagery-in-terminal-and-download-download-links)
  * [3.1. Display information about Landsat imagery in terminal](#31-display-information-about-landsat-imagery-in-terminal)
  * [3.2. Display information about Landsat imagery in terminal and download download links](#32-display-information-about-landsat-imagery-in-terminal-and-download-download-links)
  * [3.3. Display information about Sentinel imagery in terminal and download query file](#33-display-information-about-sentinel-imagery-in-terminal-and-download-query-file)
- [4. Download the imagery](#4-download-the-imagery)
  * [4.1. Download Landsat imagery](#41-download-landsat-imagery)
  * [4.2. Download Sentinel imagery](#42-download-sentinel-imagery)

<br>

## 1. Download metadata of imagery

<br>

### 1.1. Download metadata of Landsat imagery (no longer functional)

*In Windows:*

`podman run -it -v "C:\Users\GIS:/home/docker/force" davidfrantz/force force-level1-landsat -s LC08 -u /home/docker/force/force/meta `

Explanation

`podman run` launches a container from the specified image i.e. FORCE

`it` will get you an interactive shell inside the container

`-v "C:\Users\GIS:/home/docker/force"` makes your local directory "GIS" with 
the file path "C:\Users\GIS" accessible inside the container "force" (which has 
the file path "/home/docker/force"). This makes it possible to share files between 
the host and the container. Here, `-v` stands for volume mount.

`davidfrantz/force` specifies that we are using the 'force' image from user davidfrantz
on Docker Hub.

`force-level1-landsat` is a submodule within the Level 1 Archiving Suite module of FORCE.
As stated by its [official documentation here][1]: " `force-level1-landsat` offers a simple
command line interface to communicate with the USGS/EROS machine-to-machine API. The tool 
can be used to search for Landsat Collection 2 Level 1 product bundles, create download links
for the results and/or directly download the data."

`-s LC08` specifies the sensor used i.e. Landsat 8 Operational Land Imager (OLI). You can 
find the list of other sensors in the [official FORCE tutorial for data querying and download][2].

`/home/docker/force/force/meta` specifies the directory where the queried metadata should be 
downloaded to, inside the container, which corresponds to "/force/meta" directory within Windows. 
So. after download, the metadata can found be under "C:\Users\GIS\force\meta". Just ensure that 
there is a folder called "force" under ""C:\Users\GIS"

A file called "index.csv" would be downloaded, containing the metadata of all Landsat imagery 
till 2021 end. The file size till 2021-12-31 is 2.44 GB.

<br>

### 1.2. Download metadata of Sentinel imagery

*In Linux:*

Create the download folder titled "meta_sentinel":

`sudo docker run -it -v /home/GIS/force2:/data davidfrantz/force mkdir /data/meta_sentinel`

Here the `mkdir` command creates the folder/directory.

Download the metadata:

`sudo docker  run -it -v /home/GIS/force2:/data davidfrantz/force force-level1-csd -u /data/meta_sentinel`

Output
```
Downloading Sentinel-2 metadata catalogue...
Extracting compressed Sentinel-2 metadata catalogue...

Downloading and extracting tile / footprint shapefiles...

Done. You can run this script without option -u to download data now.
```
The file size of the Sentinel metadata when downloaded on 16th September 2025 was 12.4 GB 
(Ubuntu 24.04.3 LTS shows file size in Gigabytes)

*In Windows:*

`podman run -it -v "C:\Users\GIS:/home/docker/force" davidfrantz/force force-level1-csd -u /home/docker/force/force/meta_sentinel`

Output:

```
Downloading Sentinel-2 metadata catalogue...
Extracting compressed Sentinel-2 metadata catalogue...
gzip: /home/docker/force/force/meta2/index.csv: Operation not permitted
gzip: /home/docker/force/force/meta2/index.csv: Operation not permitted
```
In case of the above error, use the following the `–user root` flag before the file directory. 
The file should still be downloaded in both the cases.

`podman run -it --user root -v "C:\Users\GIS:/home/docker/force" davidfrantz/force force-level1-csd -u /home/docker/force/force/meta_sentinel`
 
The file size of the Sentinel metadata when downloaded on 15th September 2025 was 11.5 GiB 
(Windows 10 and 11 shows file size in Gibibytes). 

<br>

## 2. R script for viewing and understanding Landsat and Sentinel metadata

Open R Studio and paste the following code in an R script. Or alternatively, you can 
[download the script from here][3].

```
#View the metadata for Landsat imagery (same workflow can be used for understanding 
#Sentinel imagery too, but with modifications as Sentinel as has different column names):

#Check working directory:
getwd()

#Ctrl+Shift+H for setting working directory using GUI
#Or use setwd("~/force") (relative path) or setwd("C:/Users/GIS/force") (full path)

#Need packages:
packages <- c("data.table","dplyr")

#Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# ** libs
# sh: 1: make: not found
# Error in `<current-expression>` : error in running command
# * removing ‘/home/GIS/R/x86_64-pc-linux-gnu-library/4.5/utf8’

#If you get errors like above while running in Linux, run this in the terminal:
#sudo apt-get install build-essential
#This installs 'make' which is necessary for the installation of packages.

#Load the packages:
invisible(lapply(packages, library, character.only = TRUE))

#library(data.table)

#For landsat
#Use fread() from the data.table package instead of read.csv from base R because
#the former is more memory-efficient
landsat_meta <- fread("meta/index.csv")
#See the total 6 rows of the data:
head(landsat_meta)

summary(landsat_meta$DATE_ACQUIRED)
# Min.      1st Qu.       Median         Mean      3rd Qu.         Max. 
# "1972-07-25" "1992-05-09" "2005-11-23" "2003-09-26" "2015-08-22" "2021-12-31" 

max(landsat_meta$DATE_ACQUIRED)
#[1] "2021-12-31"

#So, the latest data for which Landsat imagery is available in the metadata is 31st December 2021

#Remove the Landsat metadata from the environment
rm(landsat_meta)

#WARNING: DON'T RUN THE NEXT LINE OF CODE UNLESS YOU HAVE AT LEAST A 32 GB RAM, 
#AND IDEALLY 64 OR ABOVE, AS THE SENTINEL METADATA TILL APRIL END, 2025 IS 11.5 GB
#WHICH TRANSLATES TO AROUND 22.6 GB IN R ENVIRONMENT

#Change the directory to the Sentinel data is stored if required
sentinel_meta <- fread("meta_sentinel/metadata_sentinel2.csv")
#OR (depending on which version of FORCE you're using)
sentinel_meta <- fread("meta_sentinel/index.csv")

#See the total 6 rows of the data:
head(sentinel_meta)
#See the structure of the data:
str(sentinel_meta)

summary(sentinel_meta$SENSING_TIME)

# Min.                    1st Qu.                     Median                       Mean 
# "2015-06-28 08:13:27.9089" "2019-02-23 22:33:59.5000" "2021-01-15 15:13:51.0065" "2020-12-05 04:07:43.3391" 
# 3rd Qu.                       Max.                       NA's 
# "2022-11-09 08:41:13.6712" "2024-08-27 08:18:37.2850"                   "954964" 

#OR

# Min.               1st Qu.                Median                  Mean               3rd Qu. 
# "2015-06-28 08:13:27" "2019-02-23 22:33:59" "2021-01-15 15:13:51" "2020-12-05 04:07:43" "2022-11-09 08:41:13" 
# Max.                  NA's 
# "2024-08-27 08:18:37"              "954964" 

#So, the latest data for which Sentinel imagery is available in the metadata is 27th August 2024

#library(dplyr)

#Filter all the imagery to those containing less than 10% cloud cover:
sent_cl_filt <- sentinel_meta %>% 
  filter(CLOUD_COVER < 10)
head(sent_cl_filt)

#Remove the main meta data file to save memory:
rm(sentinel_meta)

#Free unused R memory
gc()

#Filter to the year 2024:
sent_yr_filt <- sent_cl_filt %>% 
  filter(grepl('2024', SENSING_TIME))
#Check to verify if years have been filtered
head(sent_yr_filt)

#Remove the cloud-filtered meta data file to save memory:
rm(sent_cl_filt)
#Free unused R memory
gc()

#View the full file containing cloud- and year-filtered Sentinel metadata:
View(sent_yr_filt)

#Check the number of Sentinel imagery in 2024 containing cloud-cover less than 10%:
nrow(sent_yr_filt)
#A: 852081 imagery
```
<br>

## 3. Display information about satellite imagery in terminal and download download links

<br>

### 3.1. Display information about Landsat imagery in terminal

<br>

*Example A.* To print information on terminal, of those images between 1st Jan 2024 to 31st
December 2024 having cloud over between 0 and 10 %, and only from the OLI sensor with no 
download links generated using the shapefile titled study_area.shp, run the following in 
terminal/command-line-interface: 

<br>

*In Linux:*

First create the necessary output folder "level1":
`sudo docker run -it -v /home/GIS/force2:/data davidfrantz/force mkdir /data/level1`

Then run the command to display the information:

`sudo docker run -it -v /home/GIS/force:/data davidfrantz/force force-level1-landsat search -s OLI -d 20240101,20241231 -c 0,10 --no-action --secret /data/text.txt /data/study_area/study_area.shp /data/level1`

Here, the -n flag stands for "no action" and will result in no download of the images, 
but only retrieve the number of images. -c flag is for the cloud cover and -d is for the 
date range. Learn about all the possible options in FORCE's official tutorial [here][4]. 
force/level1 is the directory to the output directory (not used for just display). 
`--secret /home/docker/force/force/text.txt` is for path to the file containing the username
and application token for M2MApi access, which can be got after registering with the 
[USGS portal][5]. The "study_area.shp" is the shapefile of the study area of interest 
located along with their associated files such as "study_area.shx", "study_area.dbf", 
and "study_area.prj" within the folder "study_area" (within the "force" directory).

The .txt should be of the following structure (the app token below is a dummy one and 
won't work), where Paul_Pop is the USGS username and the third line the app-token:

```
app-token
Paul_Pop
I@WKuR5XM!M84aTf_dsST2lV5_x56oOpEIXhsAVT7tG5Ast9c@gDQ6zSeS17jZ!6
```

Output:

```
Sensor(s): OLI
Tile(s): 134040,134041,135040,135041,136040
Date range: 2024-01-01 to 2024-12-31
Included months: 1,2,3,4,5,6,7,8,9,10,11,12
Cloud cover: 0% to 10%

44 Landsat Level 1 scenes matching criteria found
51.61 GB data volume found
```

<br>

*In Windows:*

In case of Windows, when creating the .txt file directly from an editor, the 
End-of-line (EOL) formatting will be that of Windows (CRLF), as seen in the below image:

![enter image description here][6]

This will be not read by the FORCE software as it is made for Linux. So, the file
EOL has to be changed to Unix (LF) style. This can be achieved by the following steps
in the PowerShell:

First open the WSL2 terminal using the following command:

`wsl`

Ouput:

```
Welcome to Ubuntu 22.04.5 LTS (GNU/Linux 5.15.167.4-microsoft-standard-WSL2 x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

 System information as of Wed Sep 17 16:10:21 IST 2025

  System load:  0.0                Processes:             90
  Usage of /:   2.3% of 250.92GB   Users logged in:       0
  Memory usage: 5%                 IPv4 address for eth0: 172.27.150.78
  Swap usage:   0%
```

Now run the following after changing the username and token name to your own:

`printf 'app-token\nPaul_Pop\nI@WKuR5XM!M84aTf_dsST2lV5_x56oOpEIXhsAVT7tG5Ast9c@gDQ6zSeS17jZ!6\n' > "/mnt/c/Users/GIS/force/text.txt"`

The `\n` moves the succeeding text to the next line. The above command will create the
.txt file you need with the EOL formatting as Unix (LF).

Then write the command `exit` to exit the WSL terminal.

Output:

`logout`

Alternatively, if you have Visual Studio Code installed in your system, you can simply
create the .txt file in any text file editor in Windows, and then right click and open 
the file in Visual Studio code.

![enter image description here][7]

Then change the EOL formatting by clicking on CRLF on the bottom right and changing it
to LF from the drowndown menu on the top.

![enter image description here][8]


Then run the command to display the information:

PS C:\Users\GIS> `podman run -it -v "C:\Users\GIS:/home/docker/force" davidfrantz/force force-level1-landsat search -s OLI -n -c 0,10 -d 20240101,20241231 /home/docker/force/force/study_area/study_area.shp /home/docker/force/force/level1 --secret /home/docker/force/force/text.txt`

Output:

```
Sensor(s): OLI
Tile(s): 134040,134041,135040,135041,136040
Date range: 2024-01-01 to 2024-12-31
Included months: 1,2,3,4,5,6,7,8,9,10,11,12
Cloud cover: 0% to 10%

44 Landsat Level 1 scenes matching criteria found
51.61 GB data volume found
```


<br>

*Example B.*  For covering only those months (June, July, and August) during the 
summer time - that has melted snow and less topographic distortions.

<br>

*In Linux:*

`sudo docker run -it -v /home/GIS/force:/data davidfrantz/force force-level1-landsat search -s OLI -d 20240101,20241231 -c 0,10 -n -m 6,7,8 --secret /data/text.txt /data/study_area/study_area.shp /data/level1`

-m is the flag for selected months where the numbers correspond to the month numbers.

Output:
```
Sensor(s): OLI
Tile(s): 134040,134041,135040,135041,136040
Date range: 2024-01-01 to 2024-12-31
Included months: 6,7,8
Cloud cover: 0% to 10%

1 Landsat Level 1 scenes matching criteria found
1.21 GB data volume found
```
That means only image is found satisfying these criteria within those months.
So, its best to take imagery from all months.

<br>

*In Windows*

`podman run -it -v "C:\Users\GIS:/home/docker/force" davidfrantz/force force-level1-landsat search -s OLI -n -c 0,10 -m 6,7,8 -d 20240101,20241231 /home/docker/force/force/study_area/study_area.shp /home/docker/force/force/level1 --secret /home/docker/force/force/text.txt`

Output:
```
Sensor(s): OLI
Tile(s): 134040,134041,135040,135041,136040
Date range: 2024-01-01 to 2024-12-31
Included months: 6,7,8
Cloud cover: 0% to 10%

1 Landsat Level 1 scenes matching criteria found
1.21 GB data volume found
```

<br>

### 3.2. Display information about Landsat imagery in terminal and download download links

<br>

*Example A:* For only those imagery in June, July, and August satisfying the same criteria 
as above

*In Linux:*

`sudo docker run -it -v /home/GIS/force:/data davidfrantz/force force-level1-landsat search -s OLI -c 0,10 -m 6,7,8 -d 20240101,20241231 --secret /data/text.txt /data/study_area/study_area.shp /data/level1`

Compared to 3.1, the above command doesn't have the -n flag. This allows for the download 
links to be generated and downloaded. 

Output:

```
Sensor(s): OLI
Tile(s): 134040,134041,135040,135041,136040
Date range: 2024-01-01 to 2024-12-31
Included months: 6,7,8
Cloud cover: 0% to 10%

1 Landsat Level 1 scenes matching criteria found
1.21 GB data volume found
Writing download links to /data/level1/urls_landsat_OLI_20250710T071934.txt
```

<br>

*In Windows:*

`podman run -it -v "C:\Users\GIS:/home/docker/force" davidfrantz/force force-level1-landsat search -s OLI -c 0,10 -m 6,7,8 -d 20240101,20241231 --secret /home/docker/force/force/text.txt /home/docker/force/force/study_area/study_area.shp /home/docker/force/force/level1`

Output:

```
Sensor(s): OLI
Tile(s): 134040,134041,135040,135041,136040
Date range: 2024-01-01 to 2024-12-31
Included months: 6,7,8
Cloud cover: 0% to 10%

1 Landsat Level 1 scenes matching criteria found
1.21 GB data volume found
Writing download links to /home/docker/force/force/level1/urls_landsat_OLI_20250319T062653.txt
```
<br>

*Example B.* For all months

*In Linux:*

`sudo docker run -it -v /home/GIS/force:/data davidfrantz/force force-level1-landsat search -s OLI -c 0,10 -d 20240101,20241231 --secret /data/text.txt /data/study_area/study_area.shp /data/level1`

Output:

```
Sensor(s): OLI
Tile(s): 134040,134041,135040,135041,136040
Date range: 2024-01-01 to 2024-12-31
Included months: 1,2,3,4,5,6,7,8,9,10,11,12
Cloud cover: 0% to 10%

44 Landsat Level 1 scenes matching criteria found
51.61 GB data volume found
Writing download links to /data/level1/urls_landsat_OLI_20250710T072814.txt
```

*In Windows:*

`podman run -it -v "C:\Users\GIS:/home/docker/force" davidfrantz/force force-level1-landsat search -s OLI -c 0,10 -d 20240101,20241231 --secret /home/docker/force/force/text.txt /home/docker/force/force/study_area/study_area.shp /home/docker/force/force/level1`

Output:
```
Sensor(s): OLI
Tile(s): 134040,134041,135040,135041,136040
Date range: 2024-01-01 to 2024-12-31
Included months: 1,2,3,4,5,6,7,8,9,10,11,12
Cloud cover: 0% to 10%

44 Landsat Level 1 scenes matching criteria found
51.61 GB data volume found
2 product bundles found in output directory, 43 not downloaded yet.
Remaining download size: 50.4 GB
Writing download links to /home/docker/force/force/level1/urls_landsat_OLI_20250319T074910.txt
```

Note that in the above example for Windows, it is a step taken after download links for 
Landsat imagery from Example A of 3.2 had been downloaded. So, only the rest of the download 
links are downloaded, preventing additional redundant downloads. FORCE is smart and efficient
in this manner.

<br>

### 3.3. Display information about Sentinel imagery in terminal and download query file

We will use the CDSE downloader for this. [Here is the official documentation for the same][9]. 
We will print information of those Sentinel images between 1st Jan 2024 to 31st December 2024 
having cloud over between 0 and 10 %, and only from the S2A,S2B and S2C sensor with a json 
download query file generated using the shapefile "study_area.shp".

<br>

*In Linux:*

```
sudo docker run -it --rm \
    -v /home/GIS/force2:/data \
    vudongpham/cdse-s2 cdse-search \
    --daterange 20240101,20241231 --cloudcover 0,10 \
    /data/study_area2/study_area2.shp \
    /data
```

Note that this is a different study area (which is smaller).

Output:
```
Search all Sentinel-2 A,B scenes:
 - From 2024-01-01 to 2024-12-31
 - 0 =< Cloud cover <= 10
 - AOI : /data/study_area2/study_area2.shp
Total records: 107
Total data volume: 65.18 GB
Saved to /data/query_20250916T080027.json
```

<br>

*In Windows:*

``podman run -it -v "C:\Users\GIS:/home/docker/force" vudongpham/cdse-s2 cdse-search -d 20240101,20241231 -c 0,10 `
    /home/docker/force/force/study_area/study_area.shp `
    /home/docker/force/force``

Output:

```
Search all Sentinel-2 A,B scenes:
 - From 2024-01-01 to 2024-12-31
 - 0 =< Cloud cover <= 10
 - AOI : /home/docker/force/force/study_area/study_area.shp
Total records: 186
Total data volume: 111.91 GB
Sentinel-2 tiles: 46RES,46RET,46RFR,46RFS,46RFT,46RGR,46RGS,46RGT
Saved to /home/docker/force/force/level1_sentinel/query_20250502T075340.json
```
To check if the tiles match the shapefile, one can go to [this link][10] and compare 
the tile names.
![enter image description here][11]

To better visualise and find the tiles that match, download the shapefiles of the tiles 
[here][12] or [here][13] and compare it within the study area site in QGIS. You may already
have the shapefiles from the download of Sentinel metadata download in section 1.2:

![enter image description here][14]

The above study area of Siang valley covers the 8 tiles contained in the results of the 
search: 46RES,46RET,46RFR,46RFS,46RFT,46RGR,46RGS,46RGT

<br>

## 4. Download the imagery

<br>

### 4.1. Download Landsat imagery

*Example A:* For downloading images using the product bundle urls from the 3.2 Example A.
Remember the replace the "urls_landsat_OLI_20250319T062653.txt" with yours.

*In Windows:* 

`podman run -it -v "C:\Users\GIS:/home/docker/force" davidfrantz/force force-level1-landsat download /home/docker/force/force/level1/urls_landsat_OLI_20250319T062653.txt /home/docker/force/force/level1`

Output:
```
Loading urls from /home/docker/force/force/level1/urls_landsat_OLI_20250319T062653.txt

Found 1 product bundle URLs.
Downloading: 100%|======================================================================================================================================| 1/1 [02:55<00:00, 175.86s/product bundle]
Download complete
```
It took 2 minutes 55 minutes to download 1.21 GB of imagery in system with 32 GB RAM and 32 cores
(Windows OS). But it only downloaded imagery for a portion of the study area likely because of 
all the filters.

Note that in the above command, the flag -q was not added which means a queue file was not created. 
The -q flag will append the download urls to the queue file. Without a queue file, more processing
in FORCE is not possible.

<br>

*Example B:* For bulk download of all the imagery in the product bundle

*In Linux:*

`sudo docker run -it -v /home/GIS/force:/data davidfrantz/force force-level1-landsat download -q /data/queue.txt /data/level1/urls_landsat_OLI_20250710T072814.txt /data/level1`

`[sudo] password for GIS:` 

Output:
```
Loading urls from /data/level1/urls_landsat_OLI_20250710T072814.txt

Found 44 product bundle URLs.
Downloading: 100%|==================| 44/44 [52:00<00:00, 70.93s/product bundle]
Download complete
```
52:00 is the 52 minutes that it took to download the imagery i.e 70.93 seconds per imagery/bundle.

*In Windows:*

`podman run -it -v "C:\Users\GIS:/home/docker/force" davidfrantz/force force-level1-landsat download -q /home/docker/force/force/queue.txt /home/docker/force/force/level1/urls_landsat_OLI_20250319T120951.txt /home/docker/force/force/level1`

Output:
```
Loading urls from /home/docker/force/force/level1/urls_landsat_OLI_20250319T120951.txt

Found 43 product bundle URLs.
Downloading: 100%|=====================================================================================================================================| 43/43 [28:55<00:00, 40.36s/product bundle]
Download complete
```

28:55 is the 28 minutes and 55 seconds that it took to download the imagery i.e 40.36 seconds
per imagery/bundle.

<br>

### 4.2. Download Sentinel imagery

For Sentinel Download, download using the Level 1 Cloud Storage Downloader as stated in 
[the official FORCE documentation][15] requires gsutil config, but gsutil (Google Cloud 
Storage utility) no longer works inside a Podman container as I encountered an authentication
issue. The Problem is that Google deprecated the old gsutil config authentication method as of
February 1, 2023. The current method requires using gcloud auth login from the Google Cloud SDK. 
A solution would be to install Cloud SDK in the container, but this too didn't work.

CDSE downloader to the rescue. We can use the CDSE downloader to download Sentinel imagery. 
The json file downloaed in section 3.3 can be used for the download.

<br>

*In Linux:*

Create the download folder:

`sudo docker run -it -v /home/GIS/force2:/data davidfrantz/force mkdir /data/level1_sentinel`

Create the secret file containing the user name and password for Copernicus Data System Ecosystem.
The "secret_sentinel.txt" file in the below command is similar to the file used for the Landsat
download, except it is for the download from CDSE. It is of this format:

```
paul_pop@gis.org
$32312%gis
```
where the first line is the username and the second name is the password for your account in
CDSE. You can [register here][16].

Download the imagery:

```
sudo docker run -it --rm \
    -v /home/GIS/force2:/data \
    vudongpham/cdse-s2 cdse-download \
    /data/query_20250502T075340.json \
    /data/level1_sentinel \
    /data/secret_sentinel.txt
```
<br>

*In Windows:*

``podman run -it -v "D:\RS_GIS_files:/home/docker/force" vudongpham/cdse-s2 cdse-download `
    /home/docker/force/force/query_20250502T075340.json `
    /home/docker/force/force/level1_sentinel `
    /home/docker/force/force/secret_sentinel.txt``
    
    
<br>    

Creat the queue file:

Since the download of Sentinel imagery is done via the CDSE downloader, a queue file, like 
that in FORCE is not created. So, create it using an R script:
```
#Press Ctrl+Shift+H to interactively choose the wanted directory
#OR (in Windows) copy (but don't paste) the directory path, which will look like this: 
#D:/GIS/force/level1_sentinel and 
#run the next line of code
FolderPath <- normalizePath(readClipboard(), "/")
#set the working directory
setwd(FolderPath) #Just replace 'FolderPath' with the file path in quotes in case of using Linux

getwd() #To know which is the current working directory (optional)

#Change the the path in quotes with your own directory path
#See the files in the folder (optional) 
list.files()

#Get the names (and extensions) of all the files present in the directory: 
file_names <- list.files()

#Get the text in the required format for the queue file:
names <- paste0("/home/docker/force/force/level1_sentinel/",file_names," QUEUED")

#View 10 rows of data from the created object:
head(names, 10)
class(names)

#Convert to data frame:
names <- as.data.frame(names)

#Delete the first row as column headers won't be used in the queue file:
names(names) <- NULL

#Change working directory to where the queue file is to be saved:
#(For Windows) Copy-paste the path url and then run the following:
FolderPath <- normalizePath(readClipboard(), "/")
#set the working directory
setwd(FolderPath) #Just replace 'FolderPath' with the file path in quotes in case of using Linux

#Save as a txt file:
write.table(names, "queue_names.txt", quote = FALSE, row.names = FALSE)

#ONLY needed for Windows/OSes where the EOL is not unix-style by default:

#Convert CRLF to LF EOL:
Text = paste(readLines("queue_names.txt"), collapse="\n")
conx = file("queue.txt", open="wb")
write(Text, conx)
close(conx)
```


  [1]: https://force-eo.readthedocs.io/en/latest/components/lower-level/level1/level1-landsat.html#force-level1-landsat
  [2]: https://force-eo.readthedocs.io/en/latest/howto/level1-csd.html
  [3]: https://github.com/paulvpop/gis-land-cover-mapping/blob/main/R_script_for_reading_and_summarising_Landsat_and_Sentinel_metadata.R
  [4]: https://force-eo.readthedocs.io/en/latest/components/lower-level/level1/level1-landsat.html#force-level1-landsat
  [5]: https://ers.cr.usgs.gov/
  [6]: https://i.ibb.co/606Y0mx9/image.png
  [7]: https://i.ibb.co/235pWvWP/Screenshot-2025-09-17-151516.png
  [8]: https://i.ibb.co/8gMg9h9c/image.png
  [9]: https://github.com/vudongpham/CDSE_Sentinel2_downloader
  [10]: https://catalogue.eatlas.org.au/geonetwork/srv/eng/catalog.search#/metadata/f7468d15-12be-4e3f-a246-b2882a324f59
  [11]: https://i.ibb.co/FkDhzG5g/image.png
  [12]: https://github.com/justinelliotmeyers/Sentinel-2-Shapefile-Index
  [13]: https://sentinel.esa.int/web/sentinel/missions/sentinel-2/data-products
  [14]: https://i.ibb.co/93GmRv3S/image.png
  [15]: https://force-eo.readthedocs.io/en/latest/howto/level1-csd.html
  [16]: https://dataspace.copernicus.eu/
